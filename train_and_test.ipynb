{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["LXT0fhFemOB_"],"authorship_tag":"ABX9TyPsf07+zQf+HuwLukckG1l5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8005300f83b94bc883b8b7f7d4ddf550":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93cd65fcf8f143a8a8eb3346d5903859","IPY_MODEL_6b87dfac434242a78eb9fadc4ec4cd6f","IPY_MODEL_8f2812cf99eb4c92ae3bbe8d9f0c1dc1"],"layout":"IPY_MODEL_d0a25dbd30b043c3849e7af16809da63"}},"93cd65fcf8f143a8a8eb3346d5903859":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7282949ee204ef89ba1249783c7f61e","placeholder":"​","style":"IPY_MODEL_6b6b126272ab41dc91e2fb958c422ff8","value":"Downloading builder script: 100%"}},"6b87dfac434242a78eb9fadc4ec4cd6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea0d47b3bfe447ec826a1a980956c4dd","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fc5b1ff797d46d09c1631085830400d","value":4203}},"8f2812cf99eb4c92ae3bbe8d9f0c1dc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e48408646a464329834dbca5eb50e4db","placeholder":"​","style":"IPY_MODEL_da19f196ce604530820226fea98c7f84","value":" 4.20k/4.20k [00:00&lt;00:00, 115kB/s]"}},"d0a25dbd30b043c3849e7af16809da63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7282949ee204ef89ba1249783c7f61e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6b126272ab41dc91e2fb958c422ff8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea0d47b3bfe447ec826a1a980956c4dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fc5b1ff797d46d09c1631085830400d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e48408646a464329834dbca5eb50e4db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da19f196ce604530820226fea98c7f84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"688dca067f2043ab8b7edef8dda64edd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14754d5741e9422889b0a3f418553bd4","IPY_MODEL_3f40dea5d5924f649d2a32ff46218d21","IPY_MODEL_03c4f1a3f26d4c89b06308c13283771a"],"layout":"IPY_MODEL_6cc961d53a0b48b8b4cb72b371fb5d6e"}},"14754d5741e9422889b0a3f418553bd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9e300c45def470fbb5346f998494a61","placeholder":"​","style":"IPY_MODEL_63dae75a27a8451186898c8c195f439d","value":"Downloading builder script: 100%"}},"3f40dea5d5924f649d2a32ff46218d21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b04812711f4a95a764411efa6f73a5","max":7560,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a4be6c3cf6d4336b81cdc65405ba92b","value":7560}},"03c4f1a3f26d4c89b06308c13283771a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2fea1976ec6415da817940582ac583c","placeholder":"​","style":"IPY_MODEL_675b55e9624547a0b632ce9fb27ee269","value":" 7.56k/7.56k [00:00&lt;00:00, 240kB/s]"}},"6cc961d53a0b48b8b4cb72b371fb5d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e300c45def470fbb5346f998494a61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63dae75a27a8451186898c8c195f439d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4b04812711f4a95a764411efa6f73a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4be6c3cf6d4336b81cdc65405ba92b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2fea1976ec6415da817940582ac583c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"675b55e9624547a0b632ce9fb27ee269":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5280f8c2fd043a4957fe135418c64bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d250be4506a44179c20a9e90aa0b259","IPY_MODEL_7a97648902f64ba1b467f3d8436f837c","IPY_MODEL_9f40b722dc9b46eda6b5a0b94ac08c0e"],"layout":"IPY_MODEL_4fbc3b4a7c184134bd56974dc8923c5a"}},"0d250be4506a44179c20a9e90aa0b259":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64409297c9ed4fc98cb23e14f1258657","placeholder":"​","style":"IPY_MODEL_00346b27e7de4ff4add44d1a78531c38","value":"Downloading builder script: 100%"}},"7a97648902f64ba1b467f3d8436f837c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5515e9650b23423086bde29a62f22736","max":7377,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0dff55a6b5b47399fb905b9480c8437","value":7377}},"9f40b722dc9b46eda6b5a0b94ac08c0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6d9818a7bdf4b68849e769d79565e65","placeholder":"​","style":"IPY_MODEL_d72919d72acf4164a235f16a7884eacb","value":" 7.38k/7.38k [00:00&lt;00:00, 244kB/s]"}},"4fbc3b4a7c184134bd56974dc8923c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64409297c9ed4fc98cb23e14f1258657":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00346b27e7de4ff4add44d1a78531c38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5515e9650b23423086bde29a62f22736":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0dff55a6b5b47399fb905b9480c8437":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6d9818a7bdf4b68849e769d79565e65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d72919d72acf4164a235f16a7884eacb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#BERT and RoBERTa training and testing"],"metadata":{"id":"vvLonoWGmmt7"}},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"LXT0fhFemOB_"}},{"cell_type":"code","source":["pip install datasets"],"metadata":{"id":"mbzizANOmLBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749302779260,"user_tz":-120,"elapsed":9341,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"a832f8c7-2bd9-4ed5-d6df-074d66411220"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}]},{"cell_type":"code","source":["pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYJrMOyJgvBk","executionInfo":{"status":"ok","timestamp":1749302795332,"user_tz":-120,"elapsed":16068,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"b3d8cdd2-5c24-4c98-c790-a988ad1288a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.32.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}]},{"cell_type":"code","source":["#pip install transformers accelerate"],"metadata":{"id":"JyDkulbWukkz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741536310586,"user_tz":-60,"elapsed":69920,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"be12fd6b-8705-4933-92cb-a53f1802b449"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#%run {'drive/MyDrive/magistrale/'}useful.ipynb\n","from datasets import load_dataset, load_from_disk\n","from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,TrainerCallback,AutoModel,AutoModelForSequenceClassification,BertConfig as BertConfig\n","import torch\n","import wandb\n","import evaluate\n","\n","accuracy_metric = evaluate.load('accuracy')\n","precision_metric = evaluate.load('precision')\n","recall_metric = evaluate.load('recall')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["8005300f83b94bc883b8b7f7d4ddf550","93cd65fcf8f143a8a8eb3346d5903859","6b87dfac434242a78eb9fadc4ec4cd6f","8f2812cf99eb4c92ae3bbe8d9f0c1dc1","d0a25dbd30b043c3849e7af16809da63","b7282949ee204ef89ba1249783c7f61e","6b6b126272ab41dc91e2fb958c422ff8","ea0d47b3bfe447ec826a1a980956c4dd","2fc5b1ff797d46d09c1631085830400d","e48408646a464329834dbca5eb50e4db","da19f196ce604530820226fea98c7f84","688dca067f2043ab8b7edef8dda64edd","14754d5741e9422889b0a3f418553bd4","3f40dea5d5924f649d2a32ff46218d21","03c4f1a3f26d4c89b06308c13283771a","6cc961d53a0b48b8b4cb72b371fb5d6e","b9e300c45def470fbb5346f998494a61","63dae75a27a8451186898c8c195f439d","f4b04812711f4a95a764411efa6f73a5","8a4be6c3cf6d4336b81cdc65405ba92b","a2fea1976ec6415da817940582ac583c","675b55e9624547a0b632ce9fb27ee269","d5280f8c2fd043a4957fe135418c64bb","0d250be4506a44179c20a9e90aa0b259","7a97648902f64ba1b467f3d8436f837c","9f40b722dc9b46eda6b5a0b94ac08c0e","4fbc3b4a7c184134bd56974dc8923c5a","64409297c9ed4fc98cb23e14f1258657","00346b27e7de4ff4add44d1a78531c38","5515e9650b23423086bde29a62f22736","f0dff55a6b5b47399fb905b9480c8437","d6d9818a7bdf4b68849e769d79565e65","d72919d72acf4164a235f16a7884eacb"]},"id":"0R6UVEi9nIEO","executionInfo":{"status":"ok","timestamp":1749302893700,"user_tz":-120,"elapsed":98365,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"43ed5716-4437-4516-c8dd-23481de0ba64"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8005300f83b94bc883b8b7f7d4ddf550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/7.56k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688dca067f2043ab8b7edef8dda64edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5280f8c2fd043a4957fe135418c64bb"}},"metadata":{}}]},{"cell_type":"code","source":["print(torch.cuda.is_available())\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZZjf7Ujor9y","executionInfo":{"status":"ok","timestamp":1748445919897,"user_tz":-120,"elapsed":4,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"4a779344-9146-4caf-99b7-c5412b664096"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","1\n","Tesla T4\n"]}]},{"cell_type":"markdown","source":["# Dataset\n"],"metadata":{"id":"aAiJtUUqmSiZ"}},{"cell_type":"code","source":["def import_paths_and_nlabels(dataset_name,model_name,paragraph_selection_strategy):\n","    if dataset_name not in ['asylex-outcome','asylex-norp','sentiment1','sentiment2']:\n","        raise ValueError('dataset name not found')\n","    if model_name not in ['bert','roberta']:\n","        raise ValueError('model name not found')\n","    if paragraph_selection_strategy not in ['first','last','rand','cas','']:\n","        raise ValueError('long_text technique not found')\n","\n","    base_dataset_path = 'drive/MyDrive/magistrale/datasets/' + model_name + '/'\n","    base_model_path = 'drive/MyDrive/magistrale/Models/'  + model_name + '/'\n","\n","    dataset_filename = {'asylex-norp':'norp_','asylex-outcome':'outcome_','sentiment1':'sentiment1','sentiment2':'sentiment2'}\n","    model_filename = {'bert' : 'BERT512-', 'roberta' : 'RoBERTa512-'}\n","    model_filename2 = {'asylex-norp':'norp_','asylex-outcome':'out_','sentiment1':'sentiment1','sentiment2':'sentiment2'}\n","\n","    if dataset_name == 'asylex-norp':\n","        num_labels = 7\n","    else:\n","        num_labels = 2\n","\n","    dataset_path = base_dataset_path+ dataset_name+ '/' + dataset_filename[dataset_name] + paragraph_selection_strategy + '_'\n","    model_path = base_model_path + dataset_name + '/' + model_filename[model_name] + model_filename2[dataset_name] +paragraph_selection_strategy\n","    return dataset_path, model_path, num_labels"],"metadata":{"id":"wDpSJJMigDrN","executionInfo":{"status":"ok","timestamp":1749303523354,"user_tz":-120,"elapsed":19,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["dataset_name = 'asylex-outcome'\n","model_name = 'bert'\n","paragraph_selection_strategy = 'first'\n","\n","dataset_path, trained_model_path, n_labels  = import_paths_and_nlabels(dataset_name,model_name,paragraph_selection_strategy)\n","train_set = load_from_disk(dataset_path+'train_set')\n","test_set = load_from_disk(dataset_path+'test_set')\n","validation_set = load_from_disk(dataset_path+'validation_set')"],"metadata":{"id":"m6H71By-hQWU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"G-aeLj0LmWN-"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n","model = AutoModelForSequenceClassification.from_pretrained(trained_model_path, num_labels=n_labels)"],"metadata":{"id":"Pr1aZvnYhe5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_labels)"],"metadata":{"id":"zK-cI8Jtmd2o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742772665652,"user_tz":-60,"elapsed":359,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"875203e2-f605-43f9-cb1d-fb72fb6abcb2","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading file chat_template.jinja from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=n_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kM6-OhB_ydvO","executionInfo":{"status":"ok","timestamp":1742816427556,"user_tz":-60,"elapsed":367,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"1a394a6f-9f81-4ff5-da3e-e0a1a74f632e","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n","loading file chat_template.jinja from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.49.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["my_lr = 5e-6\n","my_epochs = 3"],"metadata":{"id":"Ib50RtEeufUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.init(\n","    project=\"Metrics on test set evaluation\",\n","    name= dataset_name+'_'+model_name+'_'+paragraph_selection_strategy,\n","    config={\n","        \"learining_rate\":my_lr,\n","        \"batch_size\":16,\n","        \"epochs\":my_epochs,\n","        \"note\":\"\"\n","    }\n",")"],"metadata":{"id":"fGdc2hNQpRZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels)\n","    recall = recall_metric.compute(predictions=predictions, references=labels)\n","    results = {**accuracy,**precision,**recall}\n","    return results\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",          # Cartella per salvare i modelli\n","    eval_strategy=\"epoch\",    # Valutare dopo ogni epoca\n","    learning_rate=my_lr,             # Learning rate\n","    per_device_train_batch_size=16, # Dimensione batch per training\n","    per_device_eval_batch_size=16,  # Dimensione batch per valutazione\n","    num_train_epochs=my_epochs,             # Numero di epoche\n","    weight_decay=0.01,              # Regularizzazione\n","    logging_dir=\"./logs\",           # Cartella per i log\n","    #logging_steps=3,\n","    logging_strategy=\"epoch\",\n","    logging_first_step = True,\n","    log_level = \"info\",\n","    report_to = \"wandb\",\n","    save_strategy=\"epoch\",\n","    save_safetensors= False# Salvataggio del modello dopo ogni epoca\n",")\n","\n","# Definire il trainer\n","\n","trainer = Trainer(\n","    model=model,                         # Modello BERT\n","    args=training_args,                  # Argomenti di training\n","    train_dataset=train_set,         # Dataset di training  #CHANGE IT\n","    eval_dataset=validation_set,          # Dataset di valutazione\n","    compute_metrics = compute_metrics\n",")"],"metadata":{"id":"hGHQ0KojpOgO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748448424001,"user_tz":-120,"elapsed":52,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"327252ff-baee-40c4-d7d5-8402eae4a3ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n"]}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Q9Co3gcypPs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.finish()"],"metadata":{"id":"QxDzNh_5vhbT","colab":{"base_uri":"https://localhost:8080/","height":568},"executionInfo":{"status":"ok","timestamp":1748448220584,"user_tz":-120,"elapsed":707,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"6d39a33c-1926-4e51-8919-8328a697f950"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.87434</td></tr><tr><td>eval/loss</td><td>0.2978</td></tr><tr><td>eval/model_preparation_time</td><td>0.0028</td></tr><tr><td>eval/runtime</td><td>61.9831</td></tr><tr><td>eval/samples_per_second</td><td>36.462</td></tr><tr><td>eval/steps_per_second</td><td>2.291</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_bert_first</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation/runs/2ntjwewf' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation/runs/2ntjwewf</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250528_155313-2ntjwewf/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"xJKI8CPCmb63"}},{"cell_type":"code","source":["trainer.evaluate(eval_dataset=test_set)"],"metadata":{"id":"Wi07LqCFmjbu","colab":{"base_uri":"https://localhost:8080/","height":699},"executionInfo":{"status":"ok","timestamp":1748448506334,"user_tz":-120,"elapsed":69958,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"782dce66-06fd-48b1-e58b-7d8042620d7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, complete_input_ids, first_sentence, all_sentences, id, info, tokenized_determinations, text. If decision_outcome, complete_input_ids, first_sentence, all_sentences, id, info, tokenized_determinations, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2260\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0]\n","[[ 2.394293   -2.0822196 ]\n"," [ 2.922452   -2.5275872 ]\n"," [ 2.8120449  -2.4954085 ]\n"," [ 2.9993873  -2.6536186 ]\n"," [ 2.3034241  -1.6276193 ]\n"," [ 2.0530868  -1.6227229 ]\n"," [ 2.446073   -1.8333882 ]\n"," [ 2.7447019  -2.4370067 ]\n"," [ 1.7351594  -1.205076  ]\n"," [ 2.745955   -2.4742198 ]\n"," [ 1.0995097  -0.6347982 ]\n"," [ 1.5846359  -1.1870792 ]\n"," [-1.9587151   1.1466919 ]\n"," [ 2.6337495  -2.217937  ]\n"," [-0.0993906  -0.37179032]\n"," [ 1.4243127  -1.0530494 ]\n"," [ 3.0001009  -2.6378834 ]\n"," [ 2.681757   -2.3700876 ]\n"," [-1.8172855   0.92359275]\n"," [ 2.2436287  -1.6903404 ]]\n","[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n","0.7775891341256367\n"]},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.2977994978427887,\n"," 'eval_model_preparation_time': 0.0032,\n"," 'eval_accuracy': 0.8743362831858407,\n"," 'eval_precision': 0.7775891341256367,\n"," 'eval_recall': 0.7495908346972177,\n"," 'eval_runtime': 69.9178,\n"," 'eval_samples_per_second': 32.324,\n"," 'eval_steps_per_second': 2.031}"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["save_path = \"drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas\"\n","model.save_pretrained(save_path)\n","tokenizer.save_pretrained(save_path)"],"metadata":{"id":"I9G6Kek4o8Ul","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742818048818,"user_tz":-60,"elapsed":1431,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"86552382-bfa0-4498-f832-10a99a8df28a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Configuration saved in drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/config.json\n","Model weights saved in drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/model.safetensors\n","tokenizer config file saved in drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/tokenizer_config.json\n","Special tokens file saved in drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/special_tokens_map.json\n"]},{"output_type":"execute_result","data":{"text/plain":["('drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/tokenizer_config.json',\n"," 'drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/special_tokens_map.json',\n"," 'drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/vocab.json',\n"," 'drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/merges.txt',\n"," 'drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/added_tokens.json',\n"," 'drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/tokenizer.json')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')\n","    recall = recall_metric.compute(predictions=predictions, references=labels,average='macro')\n","    results = {**accuracy,**precision,**recall}\n","    return results\n","\n","def evaluate_combination(dataset_name,model_name,paragraph_selection_strategy):\n","    dataset_path, trained_model_path, n_labels  = import_paths_and_nlabels(dataset_name,model_name,paragraph_selection_strategy)\n","    train_set = load_from_disk(dataset_path+'train_set')\n","    test_set = load_from_disk(dataset_path+'test_set')\n","    validation_set = load_from_disk(dataset_path+'validation_set')\n","\n","    tokenizer = AutoTokenizer.from_pretrained(trained_model_path)\n","    model = AutoModelForSequenceClassification.from_pretrained(trained_model_path, num_labels=n_labels)\n","\n","    my_lr = 5e-6\n","    my_epochs = 3\n","\n","    wandb.init(\n","        project=\"Metrics on test set evaluation3\",\n","        name= dataset_name+'_'+model_name+'_'+paragraph_selection_strategy,\n","        config={\n","            \"learining_rate\":my_lr,\n","            \"batch_size\":16,\n","            \"epochs\":my_epochs,\n","            \"note\":\"\"\n","        }\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",          # Cartella per salvare i modelli\n","        eval_strategy=\"epoch\",    # Valutare dopo ogni epoca\n","        learning_rate=my_lr,             # Learning rate\n","        per_device_train_batch_size=16, # Dimensione batch per training\n","        per_device_eval_batch_size=16,  # Dimensione batch per valutazione\n","        num_train_epochs=my_epochs,             # Numero di epoche\n","        weight_decay=0.01,              # Regularizzazione\n","        logging_dir=\"./logs\",           # Cartella per i log\n","        #logging_steps=3,\n","        logging_strategy=\"epoch\",\n","        logging_first_step = True,\n","        log_level = \"info\",\n","        report_to = \"wandb\",\n","        save_strategy=\"epoch\",\n","        save_safetensors= False# Salvataggio del modello dopo ogni epoca\n","    )\n","    trainer = Trainer(\n","        model=model,                         # Modello BERT\n","        args=training_args,                  # Argomenti di training\n","        train_dataset=train_set,         # Dataset di training  #CHANGE IT\n","        eval_dataset=validation_set,          # Dataset di valutazione\n","        compute_metrics = compute_metrics\n","    )\n","    trainer.evaluate(eval_dataset=test_set)\n","    wandb.finish()\n","\n","def new_metrics_creator():\n","    for dataset_name in ['asylex-norp','asylex-outcome','sentiment1','sentiment2']:\n","        for model_name in ['bert','roberta']:\n","            if dataset_name == 'asylex-outcome' or dataset_name == 'asylex-norp':\n","                for paragraph_selection_strategy in ['first','last','cas','rand']:\n","                    print(dataset_name + '_' + model_name +'_'+paragraph_selection_strategy)\n","                    evaluate_combination(dataset_name,model_name,paragraph_selection_strategy)\n","            else:\n","                paragraph_selection_strategy = ''\n","                print(dataset_name + '_' + model_name +'_'+paragraph_selection_strategy)\n","                evaluate_combination(dataset_name,model_name,paragraph_selection_strategy)\n","\n","new_metrics_creator()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7N0bfSiFpMXl","executionInfo":{"status":"ok","timestamp":1749304725483,"user_tz":-120,"elapsed":1146177,"user":{"displayName":"Giovanni Bergami","userId":"16330141405845647027"}},"outputId":"972760ca-4f58-4d34-ff04-e8c867336fc2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["asylex-norp_bert_first\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-bergami\u001b[0m (\u001b[33mprogetti\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134009-g6os4n0x</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/g6os4n0x' target=\"_blank\">asylex-norp_bert_first</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/g6os4n0x' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/g6os4n0x</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.78947</td></tr><tr><td>eval/loss</td><td>0.63764</td></tr><tr><td>eval/model_preparation_time</td><td>0.0029</td></tr><tr><td>eval/precision</td><td>0.74056</td></tr><tr><td>eval/recall</td><td>0.75947</td></tr><tr><td>eval/runtime</td><td>14.1902</td></tr><tr><td>eval/samples_per_second</td><td>36.152</td></tr><tr><td>eval/steps_per_second</td><td>2.326</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_bert_first</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/g6os4n0x' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/g6os4n0x</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134009-g6os4n0x/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_bert_last\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_last/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_last/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_last.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134045-tuwpvcny</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/tuwpvcny' target=\"_blank\">asylex-norp_bert_last</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/tuwpvcny' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/tuwpvcny</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.74854</td></tr><tr><td>eval/loss</td><td>0.79702</td></tr><tr><td>eval/model_preparation_time</td><td>0.0054</td></tr><tr><td>eval/precision</td><td>0.71075</td></tr><tr><td>eval/recall</td><td>0.71321</td></tr><tr><td>eval/runtime</td><td>13.7949</td></tr><tr><td>eval/samples_per_second</td><td>37.188</td></tr><tr><td>eval/steps_per_second</td><td>2.392</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_bert_last</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/tuwpvcny' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/tuwpvcny</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134045-tuwpvcny/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_bert_cas\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_cas/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_cas/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_cas.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134118-rzef0hzt</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rzef0hzt' target=\"_blank\">asylex-norp_bert_cas</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rzef0hzt' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rzef0hzt</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.78752</td></tr><tr><td>eval/loss</td><td>0.68281</td></tr><tr><td>eval/model_preparation_time</td><td>0.0029</td></tr><tr><td>eval/precision</td><td>0.74705</td></tr><tr><td>eval/recall</td><td>0.73856</td></tr><tr><td>eval/runtime</td><td>13.9126</td></tr><tr><td>eval/samples_per_second</td><td>36.873</td></tr><tr><td>eval/steps_per_second</td><td>2.372</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_bert_cas</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rzef0hzt' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rzef0hzt</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134118-rzef0hzt/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_bert_rand\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_rand/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_rand/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-norp/BERT512-norp_rand.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134150-qne71zk7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qne71zk7' target=\"_blank\">asylex-norp_bert_rand</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qne71zk7' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qne71zk7</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 240\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.98333</td></tr><tr><td>eval/loss</td><td>0.09203</td></tr><tr><td>eval/model_preparation_time</td><td>0.0029</td></tr><tr><td>eval/precision</td><td>0.98008</td></tr><tr><td>eval/recall</td><td>0.98549</td></tr><tr><td>eval/runtime</td><td>6.5357</td></tr><tr><td>eval/samples_per_second</td><td>36.722</td></tr><tr><td>eval/steps_per_second</td><td>2.295</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_bert_rand</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qne71zk7' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qne71zk7</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134150-qne71zk7/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_roberta_first\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_first/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_first/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_first.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134217-xif8tx38</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/xif8tx38' target=\"_blank\">asylex-norp_roberta_first</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/xif8tx38' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/xif8tx38</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.78363</td></tr><tr><td>eval/loss</td><td>0.67121</td></tr><tr><td>eval/model_preparation_time</td><td>0.0029</td></tr><tr><td>eval/precision</td><td>0.73622</td></tr><tr><td>eval/recall</td><td>0.76033</td></tr><tr><td>eval/runtime</td><td>13.9526</td></tr><tr><td>eval/samples_per_second</td><td>36.767</td></tr><tr><td>eval/steps_per_second</td><td>2.365</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_roberta_first</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/xif8tx38' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/xif8tx38</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134217-xif8tx38/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_roberta_last\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_last/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_last/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_last.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134251-qg11a9fx</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qg11a9fx' target=\"_blank\">asylex-norp_roberta_last</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qg11a9fx' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qg11a9fx</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73879</td></tr><tr><td>eval/loss</td><td>0.77089</td></tr><tr><td>eval/model_preparation_time</td><td>0.0068</td></tr><tr><td>eval/precision</td><td>0.69758</td></tr><tr><td>eval/recall</td><td>0.69965</td></tr><tr><td>eval/runtime</td><td>14.0224</td></tr><tr><td>eval/samples_per_second</td><td>36.584</td></tr><tr><td>eval/steps_per_second</td><td>2.353</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_roberta_last</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qg11a9fx' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/qg11a9fx</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134251-qg11a9fx/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_roberta_cas\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_cas/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_cas/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_cas.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134325-fmya3lgv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fmya3lgv' target=\"_blank\">asylex-norp_roberta_cas</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fmya3lgv' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fmya3lgv</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 513\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [33/33 00:13]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.78752</td></tr><tr><td>eval/loss</td><td>0.65708</td></tr><tr><td>eval/model_preparation_time</td><td>0.003</td></tr><tr><td>eval/precision</td><td>0.76096</td></tr><tr><td>eval/recall</td><td>0.75842</td></tr><tr><td>eval/runtime</td><td>14.1127</td></tr><tr><td>eval/samples_per_second</td><td>36.35</td></tr><tr><td>eval/steps_per_second</td><td>2.338</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_roberta_cas</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fmya3lgv' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fmya3lgv</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134325-fmya3lgv/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-norp_roberta_rand\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_rand/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_rand/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-norp/RoBERTa512-norp_rand.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134401-6mbpm0xz</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/6mbpm0xz' target=\"_blank\">asylex-norp_roberta_rand</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/6mbpm0xz' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/6mbpm0xz</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids. If decision_outcome, tokenized_sentence, NORP, sentence, text, info, id, all_sentences, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 219\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [14/14 00:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.9589</td></tr><tr><td>eval/loss</td><td>0.36696</td></tr><tr><td>eval/model_preparation_time</td><td>0.003</td></tr><tr><td>eval/precision</td><td>0.94748</td></tr><tr><td>eval/recall</td><td>0.95561</td></tr><tr><td>eval/runtime</td><td>6.0495</td></tr><tr><td>eval/samples_per_second</td><td>36.202</td></tr><tr><td>eval/steps_per_second</td><td>2.314</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-norp_roberta_rand</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/6mbpm0xz' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/6mbpm0xz</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134401-6mbpm0xz/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_bert_first\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_first/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_first/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_first.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134438-r4cxmdhm</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r4cxmdhm' target=\"_blank\">asylex-outcome_bert_first</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r4cxmdhm' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r4cxmdhm</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2260\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.87434</td></tr><tr><td>eval/loss</td><td>0.2978</td></tr><tr><td>eval/model_preparation_time</td><td>0.0028</td></tr><tr><td>eval/precision</td><td>0.84301</td></tr><tr><td>eval/recall</td><td>0.83507</td></tr><tr><td>eval/runtime</td><td>63.3323</td></tr><tr><td>eval/samples_per_second</td><td>35.685</td></tr><tr><td>eval/steps_per_second</td><td>2.242</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_bert_first</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r4cxmdhm' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r4cxmdhm</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134438-r4cxmdhm/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_bert_last\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_last/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_last/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_last.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134610-kyx79kiv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/kyx79kiv' target=\"_blank\">asylex-outcome_bert_last</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/kyx79kiv' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/kyx79kiv</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2260\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.9823</td></tr><tr><td>eval/loss</td><td>0.07801</td></tr><tr><td>eval/model_preparation_time</td><td>0.005</td></tr><tr><td>eval/precision</td><td>0.979</td></tr><tr><td>eval/recall</td><td>0.97602</td></tr><tr><td>eval/runtime</td><td>64.4471</td></tr><tr><td>eval/samples_per_second</td><td>35.068</td></tr><tr><td>eval/steps_per_second</td><td>2.203</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_bert_last</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/kyx79kiv' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/kyx79kiv</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134610-kyx79kiv/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_bert_cas\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_cas/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_cas/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_cas.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134738-h51w3k9t</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h51w3k9t' target=\"_blank\">asylex-outcome_bert_cas</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h51w3k9t' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h51w3k9t</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2260\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.90619</td></tr><tr><td>eval/loss</td><td>0.26081</td></tr><tr><td>eval/model_preparation_time</td><td>0.0053</td></tr><tr><td>eval/precision</td><td>0.89706</td></tr><tr><td>eval/recall</td><td>0.85897</td></tr><tr><td>eval/runtime</td><td>65.2585</td></tr><tr><td>eval/samples_per_second</td><td>34.631</td></tr><tr><td>eval/steps_per_second</td><td>2.176</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_bert_cas</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h51w3k9t' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h51w3k9t</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134738-h51w3k9t/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_bert_rand\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_rand/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_rand/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/asylex-outcome/BERT512-out_rand.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_134912-hr0u90zg</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/hr0u90zg' target=\"_blank\">asylex-outcome_bert_rand</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/hr0u90zg' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/hr0u90zg</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 1934\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='121' max='121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [121/121 00:55]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95915</td></tr><tr><td>eval/loss</td><td>0.21845</td></tr><tr><td>eval/model_preparation_time</td><td>0.0054</td></tr><tr><td>eval/precision</td><td>0.95757</td></tr><tr><td>eval/recall</td><td>0.92496</td></tr><tr><td>eval/runtime</td><td>55.7789</td></tr><tr><td>eval/samples_per_second</td><td>34.673</td></tr><tr><td>eval/steps_per_second</td><td>2.169</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_bert_rand</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/hr0u90zg' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/hr0u90zg</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_134912-hr0u90zg/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_roberta_first\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_first/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_first/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_first.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135032-fdvyg1ge</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fdvyg1ge' target=\"_blank\">asylex-outcome_roberta_first</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fdvyg1ge' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fdvyg1ge</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2259\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.87251</td></tr><tr><td>eval/loss</td><td>0.29351</td></tr><tr><td>eval/model_preparation_time</td><td>0.0056</td></tr><tr><td>eval/precision</td><td>0.83608</td></tr><tr><td>eval/recall</td><td>0.84553</td></tr><tr><td>eval/runtime</td><td>64.8911</td></tr><tr><td>eval/samples_per_second</td><td>34.812</td></tr><tr><td>eval/steps_per_second</td><td>2.188</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_roberta_first</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fdvyg1ge' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/fdvyg1ge</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135032-fdvyg1ge/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_roberta_last\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_last/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_last/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_last.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135204-esztbjz0</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/esztbjz0' target=\"_blank\">asylex-outcome_roberta_last</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/esztbjz0' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/esztbjz0</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2259\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97742</td></tr><tr><td>eval/loss</td><td>0.1011</td></tr><tr><td>eval/model_preparation_time</td><td>0.005</td></tr><tr><td>eval/precision</td><td>0.9716</td></tr><tr><td>eval/recall</td><td>0.97111</td></tr><tr><td>eval/runtime</td><td>64.7174</td></tr><tr><td>eval/samples_per_second</td><td>34.906</td></tr><tr><td>eval/steps_per_second</td><td>2.194</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_roberta_last</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/esztbjz0' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/esztbjz0</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135204-esztbjz0/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_roberta_cas\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_cas.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135341-r5rhjk26</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r5rhjk26' target=\"_blank\">asylex-outcome_roberta_cas</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r5rhjk26' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r5rhjk26</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 2259\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [142/142 01:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.90615</td></tr><tr><td>eval/loss</td><td>0.2626</td></tr><tr><td>eval/model_preparation_time</td><td>0.0031</td></tr><tr><td>eval/precision</td><td>0.88358</td></tr><tr><td>eval/recall</td><td>0.87633</td></tr><tr><td>eval/runtime</td><td>64.8178</td></tr><tr><td>eval/samples_per_second</td><td>34.852</td></tr><tr><td>eval/steps_per_second</td><td>2.191</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_roberta_cas</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r5rhjk26' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/r5rhjk26</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135341-r5rhjk26/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["asylex-outcome_roberta_rand\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_rand/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_rand/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/asylex-outcome/RoBERTa512-out_rand.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135508-h9l6ynlp</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h9l6ynlp' target=\"_blank\">asylex-outcome_roberta_rand</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h9l6ynlp' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h9l6ynlp</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids. If decision_outcome, first_sentence, text, info, id, all_sentences, tokenized_determinations, complete_input_ids are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 1911\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 00:54]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.96756</td></tr><tr><td>eval/loss</td><td>0.15384</td></tr><tr><td>eval/model_preparation_time</td><td>0.0049</td></tr><tr><td>eval/precision</td><td>0.95118</td></tr><tr><td>eval/recall</td><td>0.95828</td></tr><tr><td>eval/runtime</td><td>55.1924</td></tr><tr><td>eval/samples_per_second</td><td>34.624</td></tr><tr><td>eval/steps_per_second</td><td>2.174</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">asylex-outcome_roberta_rand</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h9l6ynlp' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/h9l6ynlp</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135508-h9l6ynlp/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["sentiment1_bert_\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/sentiment1/BERT512-sentiment1/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/sentiment1/BERT512-sentiment1/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/sentiment1/BERT512-sentiment1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135624-7e4m95b3</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/7e4m95b3' target=\"_blank\">sentiment1_bert_</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/7e4m95b3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/7e4m95b3</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:28]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.902</td></tr><tr><td>eval/loss</td><td>0.23483</td></tr><tr><td>eval/model_preparation_time</td><td>0.0033</td></tr><tr><td>eval/precision</td><td>0.88794</td></tr><tr><td>eval/recall</td><td>0.88388</td></tr><tr><td>eval/runtime</td><td>29.0016</td></tr><tr><td>eval/samples_per_second</td><td>34.481</td></tr><tr><td>eval/steps_per_second</td><td>2.172</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">sentiment1_bert_</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/7e4m95b3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/7e4m95b3</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135624-7e4m95b3/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["sentiment1_roberta_\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/sentiment1/RoBERTa512-sentiment1/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/sentiment1/RoBERTa512-sentiment1/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/sentiment1/RoBERTa512-sentiment1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135713-w74dlb1q</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/w74dlb1q' target=\"_blank\">sentiment1_roberta_</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/w74dlb1q' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/w74dlb1q</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.869</td></tr><tr><td>eval/loss</td><td>0.35645</td></tr><tr><td>eval/model_preparation_time</td><td>0.0031</td></tr><tr><td>eval/precision</td><td>0.85031</td></tr><tr><td>eval/recall</td><td>0.84351</td></tr><tr><td>eval/runtime</td><td>25.9666</td></tr><tr><td>eval/samples_per_second</td><td>38.511</td></tr><tr><td>eval/steps_per_second</td><td>2.426</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">sentiment1_roberta_</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/w74dlb1q' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/w74dlb1q</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135713-w74dlb1q/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["sentiment2_bert_\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/bert/sentiment2/BERT512-sentiment2/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/bert/sentiment2/BERT512-sentiment2/model.safetensors\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/bert/sentiment2/BERT512-sentiment2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135805-rbdri0ms</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rbdri0ms' target=\"_blank\">sentiment2_bert_</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rbdri0ms' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rbdri0ms</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, present_sentence, all_sentences. If text, present_sentence, all_sentences are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 199\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 00:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.92462</td></tr><tr><td>eval/loss</td><td>0.22372</td></tr><tr><td>eval/model_preparation_time</td><td>0.0035</td></tr><tr><td>eval/precision</td><td>0.92468</td></tr><tr><td>eval/recall</td><td>0.9246</td></tr><tr><td>eval/runtime</td><td>5.746</td></tr><tr><td>eval/samples_per_second</td><td>34.633</td></tr><tr><td>eval/steps_per_second</td><td>2.262</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">sentiment2_bert_</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rbdri0ms' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/rbdri0ms</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135805-rbdri0ms/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["sentiment2_roberta_\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.json\n","loading file merges.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading file chat_template.jinja\n","loading configuration file drive/MyDrive/magistrale/Models/roberta/sentiment2/RoBERTa512-sentiment2/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.52.4\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file drive/MyDrive/magistrale/Models/roberta/sentiment2/RoBERTa512-sentiment2/model.safetensors\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at drive/MyDrive/magistrale/Models/roberta/sentiment2/RoBERTa512-sentiment2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250607_135833-0dq8wsbl</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/0dq8wsbl' target=\"_blank\">sentiment2_roberta_</a></strong> to <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/0dq8wsbl' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/0dq8wsbl</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The following columns in the Evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, present_sentence, all_sentences. If text, present_sentence, all_sentences are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","\n","***** Running Evaluation *****\n","  Num examples = 199\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 00:05]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/recall</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.91457</td></tr><tr><td>eval/loss</td><td>0.24983</td></tr><tr><td>eval/model_preparation_time</td><td>0.0052</td></tr><tr><td>eval/precision</td><td>0.9146</td></tr><tr><td>eval/recall</td><td>0.9146</td></tr><tr><td>eval/runtime</td><td>5.6872</td></tr><tr><td>eval/samples_per_second</td><td>34.991</td></tr><tr><td>eval/steps_per_second</td><td>2.286</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">sentiment2_roberta_</strong> at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/0dq8wsbl' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3/runs/0dq8wsbl</a><br> View project at: <a href='https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3' target=\"_blank\">https://wandb.ai/progetti/Metrics%20on%20test%20set%20evaluation3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250607_135833-0dq8wsbl/logs</code>"]},"metadata":{}}]}]}